{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-7KD_jpJRCX"
      },
      "source": [
        "# **Разработка интеллектуального бота на основе методов NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SffRrypPLPwa"
      },
      "source": [
        "Чат-бот – это программа, участвующая в разговоре с передачей права\n",
        "голоса, целью которой является интерпретация входного текста или речи и\n",
        "вывод соответствующих полезных ответов. Сегодня почти каждая компания имеет чат-бота для взаимодействия с пользователями, ведь те, в свою очередь, обладают многими уникальными и действительно полезными функциями.\n",
        "\n",
        "Вдоховившись идеей создания подобного виртуального помощника, был разработан интеллектуальный чат-бот, в основе которого легли методы обработки естесвенного языка.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n1KxmbHkjhDy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "text1 = '''Екатерина II (Екатерина Алексеевна; Екатерина Великая; урождённая София Августа Фредерика Ангальт-Цербстская, в православии Екатерина Алексеевна; 21 апреля [2 мая] 1729, Штеттин, Пруссия — 6 ноября 1796, Зимний дворец, Санкт-Петербург, Российская империя) — императрица и Самодержица Всероссийская (1762—1796). Политик просвещённого абсолютизма.\n",
        "Дочь князя Ангальт-Цербстского, Екатерина взошла на престол в результате дворцового переворота против своего мужа — Петра III, вскоре погибшего при невыясненных обстоятельствах (возможно, он был убит). Она взошла на престол, следуя прецеденту, созданному Екатериной I, сменившей своего мужа Петра Великого в 1725 году.\n",
        "Екатерининская эпоха ознаменовалась максимальным закрепощением крестьян и всесторонним расширением привилегий дворянства.\n",
        "При Екатерине Великой границы Российской империи были значительно раздвинуты на запад (разделы Речи Посполитой) и на юг (присоединение Новороссии, Крыма, отчасти Кавказа).\n",
        "Система государственного управления при Екатерине Второй впервые со времени Петра I была реформирована. Реформы Екатерины II подготовили трансформацию русского государства и общества в первой четверти XIX века и стали необходимым условием для реформ 1860-х годов.\n",
        "В культурном отношении Россия окончательно вошла в число великих европейских держав, чему немало способствовала сама императрица, увлекавшаяся литературной деятельностью, собиравшая шедевры живописи и состоявшая в переписке с французскими просветителями. В целом политика Екатерины II и её реформы вписываются в русло просвещённого абсолютизма XVIII века.\n",
        "'''\n",
        "\n",
        "text2 = '''София Фредерика Августа Ангальт-Цербстская родилась 21 апреля (2 мая) 1729 года в немецком городе Штеттин — столице Померании (ныне — Щецин, Польша), в доме № 791 на Домштрассе.\n",
        "Отец Кристиан Август Ангальт-Цербстский происходил из цербст-дорнбургской линии Ангальтского дома и состоял на службе у прусского короля, был полковым командиром, комендантом, затем губернатором города Штеттина, где будущая императрица и появилась на свет, баллотировался в курляндские герцоги, но неудачно, службу закончил прусским фельдмаршалом. Мать — Иоганна Елизавета, из Готторпского владетельного дома, четвёртая дочь князя Гольштейн-Готторпского, после смерти отца воспитывалась при дворе своего дяди, владетельного князя Брауншвейга. Приходилась двоюродной тёткой будущему Петру III. Родословная Иоганны Елизаветы восходит к Кристиану I, королю Дании, Норвегии и Швеции, первому герцогу Шлезвиг-Гольштейнскому и основателю династии Ольденбургов.\n",
        "Дядя по материнской линии Адольф-Фридрих был в 1743 году избран в наследники шведского престола, на который он вступил в 1751 году под именем Адольфа-Фредрика. Другой дядя, Карл Эйтинский, по замыслу Екатерины I, должен был стать мужем её дочери Елизаветы, однако умер от оспы в преддверии свадебных торжеств в Санкт-Петербурге.\n",
        "В семье герцога Цербстского Екатерина получила домашнее образование. Обучалась английскому, французскому и итальянскому языкам, танцам, музыке, основам истории, географии, богословия. Она росла резвой, любознательной, шаловливой девчонкой, любила щегольнуть своей отвагой перед мальчишками, с которыми запросто играла на штеттинских улицах. Родители были недовольны «мальчишеским» поведением дочери, но их устраивало, что Фредерика заботилась о младшей сестре Августе. Мать называла её в детстве Фике или Фикхен.\n",
        "В 1743 году российская императрица Елизавета Петровна, подбирая невесту для своего наследника — великого князя Петра Фёдоровича (будущего русского императора Петра III), вспомнила о том, что на смертном одре мать завещала ей стать женой голштинского принца, родного брата Иоганны Елизаветы. Елизавета Петровна так пояснила свой выбор: «за лучшее я сочла взять принцессу протестантской веры, и при том из дома, хоть и знатного, но небольшого… Поэтому всех пригоднее принцесса Цербская, тем более что она уже в родстве с Голштинским домом». Ранее Елизавета энергично поддержала избрание на шведский престол её дяди, любекского епископа Адольфа Фридриха Голштинского, и обменялась портретами с её матерью.\n",
        "До границы Иоганна Елизавета с дочерью Софией путешествовали инкогнито как графиня Рейнбуш с дочерью. Из Берлина они выехали 31 декабря 1743 года и в конце января 1744 года пересекли русскую границу, где их встретили оружейным салютом и одарили собольими шубами. Пятнадцатилетняя принцесса с матерью проследовала в Россию через Ригу, где возле дома, в котором они остановились, нёс почётный караул поручик барон фон Мюнхгаузен. В Москву Иоганна Елизавета и София приехали 9 февраля и успели ко дню рождения великого князя (10 февраля). Впервые София увидела своего будущего мужа, Петра Фёдоровича, в Эйтинском замке в 1739 году.\n",
        "Сразу после приезда в Россию она стала изучать русский язык, историю, православие, русские традиции, так как стремилась наиболее полно ознакомиться с Россией, которую воспринимала как новую родину. Среди её учителей выделяют известного проповедника Симона Тодорского (учитель православия), автора первой русской грамматики Василия Ададурова (учитель русского языка) и балетмейстера Ланге (учитель танцев).\n",
        "Стремясь как можно быстрее выучить русский язык, будущая императрица занималась по ночам, сидя у открытого окна на морозном воздухе. Вскоре она заболела воспалением лёгких, и состояние её было столь тяжёлым, что её мать предложила привести лютеранского пастора. София, однако, отказалась и послала за Симоном Тодорским. Это обстоятельство прибавило ей популярности при русском дворе. 28 июня (9 июля) 1744 года София Фредерика Августа перешла из лютеранства в православие и получила имя Екатерины Алексеевны (то же имя и отчество, что и у матери Елизаветы — Екатерины I), а на следующий день была обручена с будущим императором.\n",
        "Появление Софии с матерью в Санкт-Петербурге сопровождалось политической интригой, в которой была замешана её мать, княгиня Цербстская. Она была поклонницей короля Пруссии Фридриха II, и последний решил использовать её пребывание при русском императорском дворе для установления своего влияния на внешнюю политику России. Для этого планировалось, посредством интриг и влияния на императрицу Елизавету Петровну, удалить от дел канцлера Бестужева, проводившего антипрусскую политику, и заменить его другим вельможей, симпатизировавшим Пруссии. Однако Бестужеву удалось перехватить письма княгини Цербстской Фридриху II и предъявить их Елизавете Петровне. После того, как последняя узнала о «некрасивой роли прусского шпиона», которую играла при её дворе мать, то немедленно изменила к ней своё отношение и подвергла опале. Однако внешне это не повлияло на положение самой Софии, официально не принимавшей участия в этой интриге.\n",
        "'''\n",
        "\n",
        "if not os.path.exists('newcorpus'):\n",
        "  os.mkdir('newcorpus')\n",
        "else:\n",
        "  pass\n",
        "\n",
        "f1 = open(os.path.join('newcorpus', 'Kate II.txt'), 'a')\n",
        "f1.write(text1)\n",
        "f1.close()\n",
        "\n",
        "f2 = open(os.path.join('newcorpus', 'Kate II Born.txt'), 'a')\n",
        "f2.write(text2)\n",
        "f2.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ-Dih1rQzUA"
      },
      "source": [
        "**Этап 1. Подключение всех необходимых библиотек**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ_L5wBQJIHu",
        "outputId": "b5224a3f-e4b7-4718-9405-24e8f925fee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: natasha in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.9.1)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: slovnet>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.6.0)\n",
            "Requirement already satisfied: yargy>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.16.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /usr/local/lib/python3.10/dist-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec>=0.9.0->natasha) (1.26.4)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install natasha\n",
        "import string #Для удаление пунктуации\n",
        "import nltk #Для загрузки пакетов\n",
        "import random #Для генерации случайно выбранного ответа\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #Для векторизации методом TF-IDF\n",
        "from sklearn.metrics.pairwise import cosine_similarity #Для посдчета косинусного сходства\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader #Для создания корпуса\n",
        "nltk.download('punkt') #Загрузка доп. пакетов\n",
        "import natasha as nt #Для нормализации текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGbp5FTQV8He"
      },
      "source": [
        "**Этап 2. Создание функций привествия и прощания с пользователем**\n",
        "\n",
        "Из истории чат-ботов известно, что каждый из них в большей или меньшей\n",
        "степени старается имитировать поведение реального человека. А что мы\n",
        "делаем обычно перед началом разговора? Приветствуем собеседника. Так и\n",
        "наш бот должен отвечать на реплики-приветствия. ELIZA использует\n",
        "простое сопоставление ключевых слов. Будем использовать ту же идею.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "46XnqZZTWWQa"
      },
      "outputs": [],
      "source": [
        "#Функция приветствия\n",
        "def welcome(user_response):\n",
        "  welcome_response = [\"Приветствую!\", \"Привет!\", \"Здравствуйте\", \"Привет, я чат-бот! Готов ответить на ваши вопросы!\"]\n",
        "  for word in user_response.split():\n",
        "      if word in welcome_input:\n",
        "          return random.choice(welcome_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IQMcr840WYg2"
      },
      "outputs": [],
      "source": [
        "#Функция прощания\n",
        "def goodbye(user_resonse):\n",
        "  goodbye_response = [\"С нетерпением буду ждать вас!\", \"До свидания!\", \"Хорошего дня!\"]\n",
        "  for word in user_response.split():\n",
        "      if word in goodbye_input:\n",
        "          return random.choice(goodbye_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEsPm3-RWrAu"
      },
      "source": [
        "**Этап 3. Создание функции для нормализации текста**\n",
        "\n",
        "Чтобы улучшить качестве обработки текста, его нужно привести к нормальному виду, а именно к его изначальной форме. Для этого воспользуемся библиотекой Natasha, которая позволит найти изначальные формы русских слов.\n",
        "В качестве параметра этой функции будем передавать текст нашего\n",
        "будущего корпуса, который нам предстоит собрать, а на выходе получать\n",
        "список токенов, которые уже прошли процесс очистки от пунктуации и\n",
        "процесс лемматизации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cSE9mKdDXLq7"
      },
      "outputs": [],
      "source": [
        "#Функция нормализации текста\n",
        "def Normalize(text):\n",
        "  #Инициализируем вспомогательные объекты библиотеки natasha\n",
        "  segmenter = nt.Segmenter()\n",
        "  morph_vocab = nt.MorphVocab()\n",
        "  emb = nt.NewsEmbedding()\n",
        "  morph_tagger = nt.NewsMorphTagger(emb)\n",
        "  ner_tagger = nt.NewsNERTagger(emb)\n",
        "\n",
        "  #Убираем знаки пунктуации из текста\n",
        "  word_token = text.translate(str.maketrans(\"\", \"\", string.punctuation)).replace(\"—\", \"\")\n",
        "\n",
        "  #Преобразуем очищенный текст в объект Doc и\n",
        "  doc = nt.Doc(word_token)\n",
        "  doc.segment(segmenter)\n",
        "  doc.tag_morph(morph_tagger)\n",
        "  doc.tag_ner(ner_tagger)\n",
        "\n",
        "  #Приводим каждое слово к его изначальной форме\n",
        "  for token in doc.tokens:\n",
        "    token.lemmatize(morph_vocab)\n",
        "  resDict = {_.text: _.lemma for _ in doc.tokens}\n",
        "\n",
        "  #Возвращаем результат в виде списка\n",
        "  return [resDict[i] for i in resDict]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDX146j5XP79"
      },
      "source": [
        "**Этап 4. Функция ответа на запрос пользователя**\n",
        "\n",
        "Формировать ответ на запрос будем методами обработки естесвенного языка. Для этого текст преобазуется в вектора методом TF-IDF, а после через косиносное сходство ищется наиболее близкий по содержанию вопроса ответ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WLOvT5SDX2RP"
      },
      "outputs": [],
      "source": [
        "#Функция ответа на запрос\n",
        "def response(user_response):\n",
        "    robo_response=''#Будущий ответ нашего бота\n",
        "    sent_tokens.append(user_response)#Временно добавим запрос пользователя в наш корпус.\n",
        "    TfidfVec = TfidfVectorizer()#Вызовем векторизатор TF-IDF\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)#Создадим вектора\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)#Через метод косинусного сходства найдем предложение с наилучшим результатом\n",
        "    idx=vals.argsort()[0][-2]#Запомним индексы этого предложения\n",
        "    flat = vals.flatten()#сглаживаем полученное косинусное сходство\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    sent_tokens.remove(user_response)#Удаляем запрос из корпуса\n",
        "    if(req_tfidf==0): #Если сглаженное значение будет равно 0, то ответ не был найден\n",
        "        robo_response=robo_response+\"Извините, я не нашел ответа...\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQqscfwpTO2P",
        "outputId": "4f0d418d-d796-4425-c99b-687874c4832f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> all\n",
            "Command 'all' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_eng to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_rus to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_ne_chunker_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Package punkt is already up-to-date!\n",
            "       | Downloading package punkt_tab to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt_tab.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package tagsets_json to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets_json.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW6RfULGYTRM"
      },
      "source": [
        "**Этап 5. Формирование основного тела**\n",
        "\n",
        "На этом создание вспомогательных функций подошло к концу. Осталось\n",
        "лишь собрать корпус и запустить интеллектуального бота, не забыв создать списки,которые будут хранить варианты возможных приветствий и прощаний\n",
        "пользователя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "wXKEeo0XYSxW",
        "outputId": "2105974c-152e-42ca-a692-c41326bcd341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Чат-бот: Привет, меня зовут Андро и я готов рассказать вам интересные факты об императрице Екатерине II!\n",
            "Чат-бот: Если вы устанете общаться со мной, то напишите мне ключевое слово 'стоп'.\n",
            "Когда Екатерина взошла на престол?\n",
            "Чат-бот: Дочь князя Ангальт-Цербстского, Екатерина взошла на престол в результате дворцового переворота против своего мужа — Петра III, вскоре погибшего при невыясненных обстоятельствах (возможно, он был убит).\n",
            "В каком году Екатерина подобрала невесту?\n",
            "Чат-бот: Екатерина II (Екатерина Алексеевна; Екатерина Великая; урождённая София Августа Фредерика Ангальт-Цербстская, в православии Екатерина Алексеевна; 21 апреля [2 мая] 1729, Штеттин, Пруссия — 6 ноября 1796, Зимний дворец, Санкт-Петербург, Российская империя) — императрица и Самодержица Всероссийская (1762—1796).\n",
            "Как Екатерина учила русский язык?\n",
            "Чат-бот: Сразу после приезда в Россию она стала изучать русский язык, историю, православие, русские традиции, так как стремилась наиболее полно ознакомиться с Россией, которую воспринимала как новую родину.\n",
            "Каковы были реформы Екатерины?\n",
            "Чат-бот: В целом политика Екатерины II и её реформы вписываются в русло просвещённого абсолютизма XVIII века.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f7faf4069933>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0muser_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0muser_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_response\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwelcome_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#Создадим корпус\n",
        "newcorpus = PlaintextCorpusReader('newcorpus/', r'.*\\.txt')\n",
        "\n",
        "data = newcorpus.raw(newcorpus.fileids())\n",
        "sent_tokens = nltk.sent_tokenize(data)\n",
        "\n",
        "#Зададим списки с возможными приветствиями и прощаниями пользователя\n",
        "welcome_input = [\"привет\", \"ку\", \"прив\", \"добрый день\", \"доброго времени суток\",\"здравствуйте\", \"приветствую\"]\n",
        "goodbye_input = [\"пока\", \"стоп\", \"выход\", \"конец\", \"до свидания\"]\n",
        "\n",
        "#Основное тело бота\n",
        "flag=True #Флаг, отвечающий за работу бота\n",
        "\n",
        "print(\"Чат-бот: Привет, меня зовут Андро и я готов рассказать вам интересные факты об императрице Екатерине II!\")\n",
        "print(\"Чат-бот: Если вы устанете общаться со мной, то напишите мне ключевое слово 'стоп'.\")\n",
        "\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "    if user_response in welcome_input:\n",
        "      print(\"Чат-бот: \"+welcome(user_response))\n",
        "    elif user_response in goodbye_input:\n",
        "      flag = False\n",
        "      print(\"Чат-бот: \"+goodbye(user_response))\n",
        "    else:\n",
        "      print(\"Чат-бот: \",end=\"\")\n",
        "      print(response(user_response))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}